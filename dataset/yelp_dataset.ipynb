{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/RUCAIBox/RecDatasets\n",
    "    \n",
    "\n",
    "!cd RecDatasets/conversion_tools\n",
    "\n",
    "os.chdir('/kaggle/working/RecDatasets/conversion_tools')\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py --dataset yelp \\\n",
    "--input_path /kaggle/input/yelp-dataset --output_path /kaggle/working/yelp \\\n",
    "--convert_inter --convert_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item=pd.read_csv('/kaggle/working/yelp/yelp.item', sep='\\t', header=None)\n",
    "df_item=df_item[[0, 1,2,3,4,11]]\n",
    "df_item.columns = ['business_id', 'item_name','address','city','state','categories']\n",
    "test_data=df_item[1:]\n",
    "tqdm.pandas()\n",
    "\n",
    "categories_dict = test_data.set_index('item_name')['categories'].to_dict()\n",
    "state = test_data.set_index('item_name')['state'].to_dict()\n",
    "city = test_data.set_index('item_name')['city'].to_dict()\n",
    "address = test_data.set_index('item_name')['address'].to_dict()\n",
    "\n",
    "\n",
    "test_data['item_text'] = test_data['item_name'].progress_apply(lambda x: f\"Title:{x} Description:{categories_dict.get(x, '')} Address:{address.get(x, '')},{city.get(x, '')},{state.get(x, '')}\")\n",
    "\n",
    "item2title = {example['business_id']: example['item_text'] for _,example in tqdm(test_data.iterrows())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making .inter file for yelp dataset\n",
    "\n",
    "df=pd.read_csv('/kaggle/working/yelp/yelp.inter', sep='\\t', header=None)\n",
    "df2=df[[1,2,3,7]]\n",
    "df2.columns = ['user_id', 'business_id','stars','date']\n",
    "df2=df2[1:]\n",
    "\n",
    "def make_inters_in_order(inters):\n",
    "    user2inters, new_inters = collections.defaultdict(list), collections.defaultdict(list)\n",
    "    for inter in inters:\n",
    "        user, item, rating, timestamp = inter\n",
    "        user2inters[user].append((user, item, rating, timestamp))\n",
    "    for user in user2inters:\n",
    "        user_inters = user2inters[user]\n",
    "        user_inters.sort(key=lambda d: d[3])\n",
    "        his_items = set()\n",
    "        for inter in user_inters:\n",
    "            user, item, rating, timestamp = inter\n",
    "            if item in his_items:\n",
    "                continue\n",
    "            his_items.add(item)\n",
    "            new_inters[user].append(inter)\n",
    "    return new_inters\n",
    "\n",
    "\n",
    "df=df2\n",
    "\n",
    "\n",
    "if len(df.columns) != 4:\n",
    "    print(f\"Warning: Expected 4 columns, but found {len(df.columns)}.\")\n",
    "    print(\"Columns found:\", df.columns)\n",
    "    print(\"First few rows of the data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "\n",
    "df.columns = ['user_id', 'item_id','rating','timestamp']\n",
    "print(\"Data loaded successfully. Shape:\", df.shape)\n",
    "print(\"Data types:\", df.dtypes)\n",
    "print(\"First few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "inters = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "    try:\n",
    "        user_id = row['user_id']\n",
    "        item_id = row['item_id']\n",
    "        timestamp = int(row['timestamp'])\n",
    "        rating=row['rating']\n",
    "        inters.append((user_id, item_id, rating, timestamp))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {row}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if not inters:\n",
    "    print(\"No valid interactions found. Check your input file format.\")\n",
    "    \n",
    "\n",
    "\n",
    "ordered_inters = make_inters_in_order(inters=inters)\n",
    "\n",
    "# Calculate timestamps for splitting (adjust these based on your data)\n",
    "all_timestamps = [inter[3] for user_inters in ordered_inters.values() for inter in user_inters]\n",
    "all_timestamps.sort()\n",
    "total_interactions = len(all_timestamps)\n",
    "valid_timestamp = all_timestamps[int(total_interactions * 0.8)]\n",
    "test_timestamp = all_timestamps[int(total_interactions * 0.9)]\n",
    "\n",
    "# Calculate timestamps for splitting (adjust these based on your data)\n",
    "all_timestamps = [inter[3] for user_inters in ordered_inters.values() for inter in user_inters]\n",
    "all_timestamps.sort()\n",
    "total_interactions = len(all_timestamps)\n",
    "valid_timestamp = all_timestamps[int(total_interactions * 0.8)]\n",
    "test_timestamp = all_timestamps[int(total_interactions * 0.9)]\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_path='/kaggle/working/data'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# For sequential recommendation\n",
    "train_file = open(os.path.join(output_path, 'train.csv'), 'w')\n",
    "valid_file = open(os.path.join(output_path, 'valid.csv'), 'w')\n",
    "test_file = open(os.path.join(output_path, 'test.csv'), 'w')\n",
    "\n",
    "train_file.write('user_id,parent_asin,rating,timestamp,history\\n')\n",
    "valid_file.write('user_id,parent_asin,rating,timestamp,history\\n')\n",
    "test_file.write('user_id,parent_asin,rating,timestamp,history\\n')\n",
    "\n",
    "for user in tqdm(ordered_inters, desc='Write seq files'):\n",
    "    cur_inter = ordered_inters[user]\n",
    "    for i in range(len(cur_inter)):\n",
    "        ts = cur_inter[i][3]\n",
    "        cur_his = ' '.join([_[1] for _ in cur_inter[:i]])\n",
    "        out_file = None\n",
    "        if ts >= test_timestamp:\n",
    "            out_file = test_file\n",
    "        elif ts >= valid_timestamp:\n",
    "            out_file = valid_file\n",
    "        else:\n",
    "            out_file = train_file\n",
    "        out_file.write(f'{cur_inter[i][0]},{cur_inter[i][1]},{cur_inter[i][2]},{cur_inter[i][3]},{cur_his}\\n')\n",
    "\n",
    "for file in [train_file, valid_file, test_file]:\n",
    "    file.close()\n",
    "\n",
    "\n",
    "splits=['valid','test','train']\n",
    "for split in tqdm(splits):\n",
    "    tf=pd.read_csv(f'/kaggle/working/data/{split}.csv')\n",
    "    tf=tf.dropna()\n",
    "    tf.to_csv(f'/kaggle/working/{split}.csv',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Final dataset creation for our task \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "import re\n",
    "import html\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings \n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Load datasets from CSV files\n",
    "train_dataset = load_dataset('csv', data_files='/kaggle/working/train.csv')['train']\n",
    "valid_dataset = load_dataset('csv', data_files='/kaggle/working/valid.csv')['train']\n",
    "test_dataset = load_dataset('csv', data_files='/kaggle/working/test.csv')['train']\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'valid': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "train_samples = 300000\n",
    "valid_samples = int((2 / 7) * train_samples)\n",
    "test_samples = int((1 / 7) * train_samples)\n",
    "\n",
    "train_subset = datasets['train'].shuffle(seed=42).select(range(min(train_samples, len(datasets['train']))))\n",
    "valid_subset = datasets['valid'].shuffle(seed=42).select(range(min(valid_samples, len(datasets['valid']))))\n",
    "test_subset = datasets['test'].shuffle(seed=42).select(range(min(test_samples, len(datasets['test']))))\n",
    "\n",
    "datasets['train'] = train_subset\n",
    "datasets['valid'] = valid_subset\n",
    "datasets['test'] = test_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "def filter_items_wo_metadata(example, item2meta):\n",
    "    if example['parent_asin'] not in item2meta:\n",
    "        example['history'] = ''\n",
    "    history = example['history'].split(' ')\n",
    "    filtered_history = [_ for _ in history if _ in item2meta]\n",
    "    example['history'] = ' '.join(filtered_history)\n",
    "    return example\n",
    "\n",
    "\n",
    "def truncate_text_history(example, max_his_len,item2meta,tokenizer,max_length=255):\n",
    "    history_items=example['history'].split(' ')\n",
    "    example['history'] = ' '.join(history_items[-max_his_len:])\n",
    "\n",
    "    start_part = \"<extra_id_0>\" #Represent <start>\n",
    "    items=[f\"<extra_id_1>Title:{item2meta[item]}<extra_id_2>\" for item in history_items] #id 1 represent item start and id 2 represent item end\n",
    "    end_part =\"<extra_id_3>\" #Represent <end>  \n",
    "    \n",
    "    # Combine parts\n",
    "    history_text = start_part + ''.join(items) + end_part\n",
    "    tokens = tokenizer.tokenize(history_text)\n",
    "\n",
    "    if len(tokens) <= max_length:\n",
    "        example['history_text'] = history_text\n",
    "        return example\n",
    "    \n",
    "    # Add items in reverse order while checking the length constraint\n",
    "    final_items = []\n",
    "    current_length = 2\n",
    "\n",
    "    for item in reversed(items):\n",
    "        new_length = current_length + len(tokenizer.tokenize(item))\n",
    "        if new_length <= max_length:\n",
    "            final_items.append(item)\n",
    "            current_length = new_length\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Ensure the items are in the original order in the final result\n",
    "    final_items.reverse()\n",
    "    final_result = start_part + ''.join(final_items) + end_part\n",
    "\n",
    "    example['history_text'] = final_result\n",
    "    return example\n",
    "\n",
    "\n",
    "def item_text(example,item2title,domain):\n",
    "    if example['parent_asin'] not in item2title:\n",
    "        example['target_item_text']=None\n",
    "        return example\n",
    "    target_item_text=item2title[example['parent_asin']]\n",
    "    example['target_item_text']=target_item_text\n",
    "    return example\n",
    "\n",
    "\n",
    "def item_desc_text(example,item2description):\n",
    "    if example['parent_asin'] not in item2description:\n",
    "        example['target_description']=None  \n",
    "        return example\n",
    "    target_description=item2description[example['parent_asin']]\n",
    "    example['target_description']=target_description\n",
    "    return example\n",
    "\n",
    "def remap_id(datasets):\n",
    "    user2id = {'[PAD]': 0}\n",
    "    id2user = ['[PAD]']\n",
    "    item2id = {'[PAD]': 0}\n",
    "    id2item = ['[PAD]']\n",
    "\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        dataset = datasets[split]\n",
    "        for user_id, item_id, history in zip(dataset['user_id'], dataset['parent_asin'], dataset['history']):\n",
    "            if user_id not in user2id:\n",
    "                user2id[user_id] = len(id2user)\n",
    "                id2user.append(user_id)\n",
    "            if item_id not in item2id:\n",
    "                item2id[item_id] = len(id2item)\n",
    "                id2item.append(item_id)\n",
    "            items_in_history = history.split(' ')\n",
    "            for item in items_in_history:\n",
    "                if item not in item2id:\n",
    "                    item2id[item] = len(id2item)\n",
    "                    id2item.append(item)\n",
    "\n",
    "    data_maps = {'user2id': user2id, 'id2user': id2user, 'item2id': item2id, 'id2item': id2item}\n",
    "    return data_maps\n",
    "\n",
    "#cleaning preprocessing dataset\n",
    "\n",
    "def list_to_str(l):\n",
    "    if isinstance(l, list):\n",
    "        return list_to_str(', '.join(l))\n",
    "    else:\n",
    "        return l\n",
    "\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    text = list_to_str(raw_text)\n",
    "    text = html.unescape(text) \n",
    "    text = text.strip()\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'[\\n\\t]', ' ', text) \n",
    "    text = re.sub(r' +', ' ', text) \n",
    "    text=re.sub(r'[^\\x00-\\x7F]', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def feature_process(feature):\n",
    "    sentence = \"\"\n",
    "    if isinstance(feature, float): \n",
    "        sentence += str(feature) \n",
    "        sentence += '.' \n",
    "    elif isinstance(feature, list) and len(feature) > 0: \n",
    "        for v in feature: \n",
    "            sentence += clean_text(v)\n",
    "            sentence += ', '\n",
    "        sentence = sentence[:-2] \n",
    "        sentence += '.'\n",
    "    else:\n",
    "        sentence = clean_text(feature)\n",
    "    return sentence + ' '\n",
    "\n",
    "\n",
    "def clean_title(example):\n",
    "    if 'title' in example and example['title']:\n",
    "        return {'parent_asin': example['parent_asin'], 'title': feature_process(example['title'])}\n",
    "    else:\n",
    "        return {'parent_asin': example['parent_asin'], 'title': None}\n",
    "\n",
    "def clean_description(example):\n",
    "    if 'description' in example and example['description']:\n",
    "        return {'parent_asin': example['parent_asin'], 'description': feature_process(example['description'])}\n",
    "    else:\n",
    "        return {'parent_asin': example['parent_asin'], 'description': None}\n",
    "\n",
    "\n",
    "\n",
    "def clean_review(example):\n",
    "    review = ''\n",
    "    features_needed = ['title', 'text']\n",
    "    for feature in features_needed:\n",
    "        if example[feature] is not None:\n",
    "            review += f'Review {feature}:' + feature_process(example[feature]) + ' '\n",
    "        else:\n",
    "            continue\n",
    "    if not review:\n",
    "        review = None\n",
    "    example['cleaned_review'] = review\n",
    "    return example\n",
    "\n",
    "def process_sentence(sentence, max_length=255):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    if len(tokens) <= max_length:\n",
    "        return sentence\n",
    "\n",
    "    flag = False\n",
    "\n",
    "    while len(tokens) > max_length:\n",
    "        start_index = sentence.find(\"<extra_id_1>\")\n",
    "        end_index = sentence.find(\"<extra_id_2>\", start_index) + len(\"<extra_id_2>\")\n",
    "\n",
    "        if start_index == -1 or end_index == -1:\n",
    "            raise AssertionError('Something wrong')\n",
    "\n",
    "        # Remove the first title\n",
    "        sentence = sentence[:start_index] + sentence[end_index:]\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "        if sentence.endswith('<extra_id_0><extra_id_3>'):\n",
    "            flag = True\n",
    "            print('bc')\n",
    "            break\n",
    "\n",
    "    if flag:\n",
    "        title_block = sentence[start_index:end_index]\n",
    "        title_tokens = tokenizer.tokenize(title_block)\n",
    "        title_token_ids = tokenizer.convert_tokens_to_ids(title_tokens)\n",
    "\n",
    "        while len(tokens) > max_length - 2 and title_token_ids:\n",
    "            title_token_ids.pop()\n",
    "            truncated_title_block = tokenizer.decode(title_token_ids, skip_special_tokens=True)\n",
    "            sentence = sentence[:start_index] + truncated_title_block + sentence[end_index:]\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "        sentence += '<extra_id_2><extra_id_3>'\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "max_his_len=50\n",
    "n_workers=64\n",
    "domain='yelp'\n",
    "output_dir = os.path.join('/kaggle/working/','domain' )\n",
    "check_path(output_dir)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Load a pre-trained Sentence Transformer model\n",
    "model = SentenceTransformer('sentence-transformers/sentence-t5-base',device=device)\n",
    "\n",
    "# Access the tokenizer from the underlying transformers model\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "truncated_datasets = {}\n",
    "\n",
    "\n",
    "print(\"Processing interaction data...\")\n",
    "for split in ['train','valid','test']:\n",
    "\n",
    "    print(f\"Processing {split} split...\")\n",
    "    filtered_dataset = datasets[split].map(\n",
    "        lambda t: filter_items_wo_metadata(t, item2title),\n",
    "        num_proc=n_workers\n",
    "    )\n",
    "    filtered_dataset = filtered_dataset.filter(lambda t: len(t['history']) > 0)\n",
    "\n",
    "    truncated_dataset = filtered_dataset.map(\n",
    "        lambda t: truncate_text_history(t, max_his_len, item2title,tokenizer),\n",
    "        num_proc=n_workers\n",
    "    )\n",
    "    truncated_dataset = truncated_dataset.map(\n",
    "        lambda t: item_text(t,item2title,domain),\n",
    "        num_proc=n_workers\n",
    "    )\n",
    "\n",
    "    truncated_datasets[split] = truncated_dataset\n",
    "    df=pd.DataFrame(truncated_datasets[split])\n",
    "    df.to_csv(os.path.join(output_dir, f'{domain}.{split}.csv'), index=False)\n",
    "\n",
    "    #For unisrec: \n",
    "    df2 = df[['user_id','history','parent_asin','timestamp']]\n",
    "    df2 = df2.rename(columns={'user_id':'user_id:token','history':'item_id_list:token_seq','parent_asin':'item_id:token','timestamp':'timestamp:float'})\n",
    "    df2.to_csv(os.path.join(output_dir, f'{domain}.{split}.inter'), sep='\\t', index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Remapping IDs...\")\n",
    "data_maps = remap_id(truncated_datasets)\n",
    "id2meta = {0: '[PAD]'}\n",
    "for item in item2title:\n",
    "    if item not in data_maps['item2id']:\n",
    "        continue\n",
    "    item_id = data_maps['item2id'][item]\n",
    "    id2meta[item_id] = item2title[item]\n",
    "data_maps['id2meta'] = id2meta\n",
    "output_path = os.path.join(output_dir, f'{domain}.data_maps')\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(data_maps, f)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Item Title\")\n",
    "sorted_text = []    # 1-base, sorted_text[0] -> item_id=1\n",
    "for i in range(1, len(data_maps['item2id'])):\n",
    "    item_text=f\"{data_maps['id2meta'][i]}\"\n",
    "    sorted_text.append(item_text)\n",
    "\n",
    "with open(os.path.join(output_dir, 'item_profile.txt'), 'w') as f:\n",
    "    for line in sorted_text:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "\n",
    "'''\n",
    "Generate Item From Unisrec - roberta\n",
    "'''\n",
    "print(\"Generating item features for Unisrec...\")\n",
    "plm = 'hyp1231/blair-roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(plm)\n",
    "model_plm = AutoModel.from_pretrained(plm).to(device)\n",
    "\n",
    "batch_size=32\n",
    "all_embeddings = []\n",
    "for pr in tqdm(range(0, len(sorted_text), batch_size)):\n",
    "    batch = sorted_text[pr:pr + batch_size]\n",
    "    inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_plm(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    all_embeddings.append(embeddings)\n",
    "all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "all_embeddings.tofile(os.path.join(output_dir, f'{domain}.{plm.split(\"/\")[-1]}.feature')) \n",
    "\n",
    "print(\"Item features generated for Unisrec.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
