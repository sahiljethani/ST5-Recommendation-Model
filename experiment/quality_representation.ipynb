{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Representation\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from cuml.manifold import TSNE\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/diss-train/data.test.csv')\n",
    "\n",
    "MODELS = {\n",
    "    'ST5-Only': 'sentence-transformers/sentence-t5-base',\n",
    "    'ST5-Final': '/kaggle/input/models/models/final_model'\n",
    "}\n",
    " \n",
    "# Define domains\n",
    "domains = ['All_Beauty', 'Baby_Products', 'Video_Games',\n",
    "           'Beauty_and_Personal_Care', 'Cell_Phones_and_Accessories',\n",
    "           'Electronics', 'Health_and_Household', 'Movies_and_TV', 'Toys_and_Games']\n",
    "\n",
    "\n",
    "# Colors for the 9 domains\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive']\n",
    "\n",
    "def get_top_users(df, domain, n=1000): #get 1000 users from each domain\n",
    "    domain_df = df[df['category'] == domain]\n",
    "    top_users = domain_df['parent_asin'].value_counts().nlargest(n).index\n",
    "    return domain_df[domain_df['parent_asin'].isin(top_users)]['history_text'].unique()[:n]\n",
    "\n",
    "def create_embeddings(model, users):\n",
    "    return model.encode(users, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "def visualize_embeddings(embeddings1, embeddings2, labels, domains, title1, title2, seed):\n",
    "    tsne = TSNE(n_components=2, random_state=seed)\n",
    "    reduced_embeddings1 = tsne.fit_transform(embeddings1)\n",
    "    reduced_embeddings2 = tsne.fit_transform(embeddings2)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 8), sharey=True)\n",
    "    \n",
    "\n",
    "    for i, domain in enumerate(domains):\n",
    "        mask = labels == i\n",
    "        axs[0].scatter(reduced_embeddings1[mask, 0], reduced_embeddings1[mask, 1], \n",
    "                       c=[colors[i]], label=domain, alpha=0.6)\n",
    "    axs[0].set_title(title1) \n",
    "\n",
    "    # Plot for the second model\n",
    "    for i, domain in enumerate(domains):\n",
    "        mask = labels == i\n",
    "        axs[1].scatter(reduced_embeddings2[mask, 0], reduced_embeddings2[mask, 1], \n",
    "                       c=[colors[i]], label=domain, alpha=0.6)\n",
    "    axs[1].set_title(title2)  \n",
    "\n",
    "    # Common legend at the bottom\n",
    "    handles, labels = axs[0].get_legend_handles_labels()  \n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=len(domains),title=\"Categories\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 1]) \n",
    "    plt.savefig('/kaggle/working/user-rep.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "all_embeddings = {model_name: [] for model_name in MODELS}\n",
    "labels = []\n",
    "\n",
    "for i, domain in enumerate(tqdm(domains, desc=\"Processing domains\")):\n",
    "    users = get_top_users(df, domain)\n",
    "    labels.extend([i] * len(users))\n",
    "\n",
    "    for model_name, model_path in MODELS.items():\n",
    "        model = SentenceTransformer(model_path)\n",
    "        embeddings = create_embeddings(model, users)\n",
    "        all_embeddings[model_name].append(embeddings.cpu().detach().numpy())\n",
    "\n",
    "for model_name in MODELS:\n",
    "    all_embeddings[model_name] = np.concatenate(all_embeddings[model_name], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "seeds = [123] \n",
    "for seed in seeds:\n",
    "    model_names = list(all_embeddings.keys())\n",
    "    visualize_embeddings(all_embeddings[model_names[0]], \n",
    "                         all_embeddings[model_names[1]], \n",
    "                         np.array(labels), domains,\n",
    "                         f\"User Representations - {model_names[0]} Model\",\n",
    "                         f\"User Representations - {model_names[1]} Model\",\n",
    "                         seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ITem Representation\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from cuml.manifold import TSNE\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv('/kaggle/input/diss-train/data.test.csv')\n",
    "\n",
    "# Define models\n",
    "MODELS = {\n",
    "    'ST5-Only': 'sentence-transformers/sentence-t5-base',\n",
    "    'ST5-Final': '/kaggle/input/models/models/final_model',\n",
    "    'UniSRec(BLAIR)': 'hyp1231/blair-roberta-base'\n",
    "}\n",
    "\n",
    "# Define domains\n",
    "domains = ['All_Beauty', 'Baby_Products', 'Video_Games',\n",
    "           'Beauty_and_Personal_Care', 'Cell_Phones_and_Accessories',\n",
    "           'Electronics', 'Health_and_Household', 'Movies_and_TV', 'Toys_and_Games']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_dir = '/kaggle/working/'\n",
    "\n",
    "# Colors for the 9 domains\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive']\n",
    "\n",
    "\n",
    "def get_items(df, domain): #get 1000 items from each domain\n",
    "    domain_df = df[df['category'] == domain]\n",
    "    texts = domain_df['target_item_text'].head(1000).drop_duplicates().tolist()\n",
    "    return texts\n",
    "\n",
    "def create_embeddings_st(model, texts):\n",
    "    return model.encode(texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "def create_embeddings_blair(model, tokenizer, texts):\n",
    "    cleaned_texts = [text.replace('<extra_id_1>', '').replace('<extra_id_2>', '') for text in texts]\n",
    "    inputs = tokenizer(cleaned_texts, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "def visualize_embeddings(embeddings, domain_labels, title, seed):\n",
    "    tsne = TSNE(n_components=2, random_state=seed)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, domain in enumerate(domains):\n",
    "        domain_mask = domain_labels == i\n",
    "        plt.scatter(reduced_embeddings[domain_mask, 0], reduced_embeddings[domain_mask, 1], \n",
    "                    c=colors[i],label=domain, alpha=0.4)\n",
    "    \n",
    "    print('SEED:',seed)\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "all_embeddings = {model_name: [] for model_name in MODELS}\n",
    "domain_labels = []\n",
    "\n",
    "# Initialize BLAIR model and tokenizer\n",
    "blair_tokenizer = AutoTokenizer.from_pretrained(MODELS['UniSRec(BLAIR)'])\n",
    "blair_model = AutoModel.from_pretrained(MODELS['UniSRec(BLAIR)']).to(device)\n",
    "\n",
    "# Calculate embeddings\n",
    "for i, domain in enumerate(tqdm(domains, desc=\"Processing domains\")):\n",
    "    texts = get_items(df, domain)\n",
    "    print(f\"{domain}: {len(texts)}\")\n",
    "    for model_name, model_path in MODELS.items():\n",
    "        if model_name == 'UniSRec(Blair)':\n",
    "            embeddings = create_embeddings_blair(blair_model, blair_tokenizer, texts)\n",
    "        else:\n",
    "            model = SentenceTransformer(model_path)\n",
    "            embeddings = create_embeddings_st(model, texts)\n",
    "        all_embeddings[model_name].append(embeddings)\n",
    "    domain_labels.extend([i] * len(texts))\n",
    "\n",
    "# Combine embeddings\n",
    "for model_name in MODELS:\n",
    "    all_embeddings[model_name] = torch.cat(all_embeddings[model_name], dim=0).cpu().detach().numpy()\n",
    "\n",
    "# Save BLAIR embeddings\n",
    "for i, domain in enumerate(domains):\n",
    "    domain_embeddings = all_embeddings['UniSRec(BLAIR)'][i*1000:(i+1)*1000]\n",
    "    file_path = os.path.join(output_dir, f'{domain}.blair.feature')\n",
    "    domain_embeddings.tofile(file_path)\n",
    "    print(f\"Saved BLAIR embeddings for {domain} to {file_path}\")\n",
    "\n",
    "# Visualize with different seeds\n",
    "seeds = [42, 123, 456] \n",
    "for seed in seeds:\n",
    "    for model_name, embeddings in all_embeddings.items():\n",
    "        visualize_embeddings(embeddings, np.array(domain_labels), f\"Item Representations - {model_name}\", seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
