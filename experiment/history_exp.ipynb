{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sasrec History Length Ablation\n",
    "\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.utils import init_seed, init_logger, get_trainer, get_model\n",
    "from recbole.evaluator import Evaluator\n",
    "from recbole.data.dataloader import FullSortEvalDataLoader\n",
    "from recbole.sampler import Sampler\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from logging import getLogger\n",
    "import numpy as np\n",
    "from recbole.data.dataloader import TrainDataLoader\n",
    "\n",
    "\n",
    "def filter_users_by_history_length(dataset, history_length=1):\n",
    "    mask = dataset.inter_feat['item_length'] == history_length\n",
    "    filtered_inter_feat = dataset.inter_feat[mask]\n",
    "    filtered_dataset = dataset.copy(filtered_inter_feat)\n",
    "    return filtered_dataset\n",
    "\n",
    "def filter_data_by_valid_items(data, n_items,config):\n",
    "    item_seqs = data.inter_feat[config['ITEM_SEQ_FIELD']]\n",
    "    \n",
    "    mask = torch.all(item_seqs < n_items, dim=1)\n",
    "    \n",
    "    filtered_inter_feat = data.inter_feat[mask]\n",
    "    \n",
    "    filtered_data = data.copy(filtered_inter_feat)\n",
    "    return filtered_data\n",
    "def sasrec_with_filtered_evaluation(domain, base_dir, history_length, model_file):\n",
    "\n",
    "    # Load model\n",
    "    if not os.path.exists(model_file):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_file}\")\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(model_file)\n",
    "        config = checkpoint[\"config\"]\n",
    "        init_seed(config[\"seed\"], config[\"reproducibility\"])\n",
    "        init_logger(config)\n",
    "        logger = getLogger()\n",
    "        logger.info(config)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model configuration: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    config['data_path']=base_dir\n",
    "    config['dataset']=domain\n",
    "    config['device'] = 'cuda'\n",
    "\n",
    "    \n",
    "    # Create dataset and prepare data\n",
    "    dataset = create_dataset(config)\n",
    "    logger.info(dataset)\n",
    "    train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "    \n",
    "       # Calculate the number of items in the dataset\n",
    "    n_items = len(dataset.inter_feat[config['ITEM_ID_FIELD']].unique())\n",
    "    print(f\"Total number of items in the dataset: {n_items}\")\n",
    "\n",
    "\n",
    "    # Apply filtering to train, valid, and test datasets\n",
    "    filtered_train_dataset = filter_data_by_valid_items(train_data.dataset, n_items,config)\n",
    "    filtered_valid_dataset = filter_data_by_valid_items(valid_data.dataset, n_items,config)\n",
    "    filtered_test_dataset = filter_data_by_valid_items(test_data.dataset, n_items,config)\n",
    "\n",
    "    # Create appropriate data loaders\n",
    "    train_sampler = Sampler(config, filtered_train_dataset)\n",
    "    train_data = TrainDataLoader(config, filtered_train_dataset, train_sampler, shuffle=True)\n",
    "    \n",
    "    valid_sampler = Sampler(config, filtered_valid_dataset)\n",
    "    valid_data = FullSortEvalDataLoader(config, filtered_valid_dataset, valid_sampler)\n",
    "    \n",
    "    test_sampler = Sampler(config, filtered_test_dataset)\n",
    "    test_data = FullSortEvalDataLoader(config, filtered_test_dataset, test_sampler)\n",
    "\n",
    "    # Filter test dataset\n",
    "    filtered_test_dataset = filter_users_by_history_length(test_data.dataset, history_length=history_length)\n",
    "\n",
    "    sampler = Sampler(config, filtered_test_dataset)\n",
    "    filtered_test_data = FullSortEvalDataLoader(config, filtered_test_dataset, sampler)\n",
    "\n",
    "    # Load model\n",
    "    try:\n",
    "        model = get_model(config[\"model\"])(config, train_data.dataset).to(config[\"device\"])\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        model.load_other_parameter(checkpoint.get(\"other_parameter\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    # Evaluation\n",
    "    trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "    test_result = trainer.evaluate(filtered_test_data, load_best_model=False, show_progress=config['show_progress'])\n",
    "\n",
    "    print(\"Final Filtered Test Results:\")\n",
    "    print(test_result)\n",
    "\n",
    "    return test_result\n",
    "\n",
    "\n",
    "\n",
    "base_dir = '/kaggle/input/data-ablation/Archive/'\n",
    "domains=['Baby_Products','Video_Games','All_Beauty']\n",
    "models=['/kaggle/input/data-ablation/SAS_Baby.pth','/kaggle/input/data-ablation/SAS_Video.pth','/kaggle/input/data-ablation/SAS_Beauty.pth']\n",
    "all_results = {domain: {} for domain in domains}\n",
    "for i,domain in enumerate(domains):\n",
    "    ncdg_10=[]\n",
    "    ncdg_50=[]\n",
    "    for history in range(2,9):\n",
    "        results = sasrec_with_filtered_evaluation(domain, base_dir+f'{domain}', history, models[i])\n",
    "        ncdg_10.append(results['ndcg@10'])\n",
    "        ncdg_50.append(results['ndcg@50'])\n",
    "    all_results[domain]['ndcg@10']=ncdg_10\n",
    "    all_results[domain]['ndcg@50']=ncdg_50\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unisrec history length ablation\n",
    "\n",
    "from logging import getLogger\n",
    "import torch\n",
    "from recbole.config import Config\n",
    "from recbole.data import data_preparation\n",
    "from recbole.utils import init_seed, init_logger, get_trainer\n",
    "from utils import get_model, create_dataset\n",
    "import os \n",
    "import json\n",
    "from recbole.evaluator import Evaluator\n",
    "from recbole.data.dataloader import FullSortEvalDataLoader\n",
    "from recbole.sampler import KGSampler, Sampler, RepeatableSampler\n",
    "\n",
    "def check_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def filter_users_by_history_length(dataset, history_length=1):\n",
    "    # Get users with the specific history length\n",
    "    mask = dataset.inter_feat['item_length'] == history_length\n",
    "    filtered_inter_feat = dataset.inter_feat[mask]\n",
    "    \n",
    "    # Create a new dataset with filtered interactions\n",
    "    filtered_dataset = dataset.copy(_)\n",
    "    filtered_dataset.inter_feat = filtered_inter_feat\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "def run(model_name,domain,base_dir,model_file,history_length):\n",
    "\n",
    "    props = ['overall.yaml', f'{model_name}.yaml']\n",
    "\n",
    "\n",
    "    # Add checkpoint_dir to kwargs\n",
    "    model_class = get_model(model_name)\n",
    "\n",
    "    # configurations initialization\n",
    "    config = Config(model=model_class, dataset=domain, config_file_list=props)\n",
    "    config['metrics'] = ['NDCG']\n",
    "    config['topk'] = [10,50]\n",
    "    config['data_path']=base_dir\n",
    "    config['dataset']=domain\n",
    "    config['device'] = 'cuda'\n",
    "\n",
    "\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "\n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(config)\n",
    "\n",
    "    # dataset creation\n",
    "    dataset = create_dataset(config)\n",
    "    logger.info(dataset)\n",
    "    print('ORIGINAL DATASET:', len(dataset.inter_feat))\n",
    "\n",
    "    # Prepare the data\n",
    "    train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "\n",
    "    # Filter users by history length in the test set\n",
    "    filtered_test_dataset = filter_users_by_history_length(test_data.dataset, history_length=history_length)\n",
    "    print(f'FILTERED TEST DATASET (history length = {history_length}):', len(filtered_test_dataset.inter_feat))\n",
    "\n",
    "    if len(filtered_test_dataset.inter_feat) == 0:\n",
    "        raise ValueError(\"No interactions left in test data after filtering. Please check your filter criteria.\")\n",
    "\n",
    "    # Create a sampler for the filtered dataset\n",
    "    sampler = Sampler(config, filtered_test_dataset)\n",
    "\n",
    "    # Create a new test_data with the filtered dataset\n",
    "    filtered_test_data = FullSortEvalDataLoader(config, filtered_test_dataset, sampler)\n",
    "\n",
    "\n",
    "    # model loading and initialization\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model_class(config, train_data.dataset).to(device)\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(filtered_test_data, load_best_model=False, model_file=model_file, show_progress=config['show_progress'])\n",
    "    print(\"Test result: \", test_result)\n",
    "    return test_result\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'UniSRec'\n",
    "domains=['All_Beauty','Video_Games','Baby_Products']\n",
    "models=['/kaggle/input/data-ablation/All_Beauty/UniSRec-Jul-22-2024_22-04-52.pth','/kaggle/input/data-ablation/Video_Games/checkpoint_unisrec/UniSRec-Jul-22-2024_23-04-56.pth','/kaggle/input/data-ablation/Baby_Products/checkpoint_unisrec/UniSRec-Jul-22-2024_22-11-11.pth']\n",
    "\n",
    "base_dir = '/kaggle/input/data-ablation/Archive/'\n",
    "\n",
    "all_results = {domain: {} for domain in domains}\n",
    "for i,domain in enumerate(domains):\n",
    "    ncdg_10=[]\n",
    "    ncdg_50=[]\n",
    "    for history in range(1,11):\n",
    "        results = run(model_name,domain,base_dir+f'{domain}',models[i],history)\n",
    "        ncdg_10.append(results['ndcg@10'])\n",
    "        ncdg_50.append(results['ndcg@50'])\n",
    "    all_results[domain]['ndcg@10']=ncdg_10\n",
    "    all_results[domain]['ndcg@50']=ncdg_50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ST5 History Length Ablation\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "device=('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = SentenceTransformer('/kaggle/input/models/models/final_model', device=device,local_files_only=True)\n",
    "#model = SentenceTransformer('sentence-transformers/sentence-t5-base', device=device)\n",
    "\n",
    "\n",
    "# # List of domains\n",
    "# domains = ['Baby_Products','Video_Games']\n",
    "\n",
    "# models=['']\n",
    "\n",
    "models=['sentence-transformers/sentence-t5-base','/kaggle/input/models/models/final_model'] #st5 only , st5 final model\n",
    "\n",
    "domains=['All_Beauty','Video_Games','Baby_Products']\n",
    "\n",
    "base_dir='/kaggle/input/recdata/data'\n",
    "\n",
    "all_results = {domain: {model: {} for model in models} for domain in domains}\n",
    "\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    tokenizer = model.tokenizer\n",
    "    for domain in domains: \n",
    "\n",
    "        print(f\"Processing domain: {domain}\")\n",
    "\n",
    "        \n",
    "        #Load Data Maps\n",
    "        data_maps_file = os.path.join(base_dir, domain, f\"{domain}.data_maps\")   \n",
    "        with open(data_maps_file, 'r') as f:\n",
    "            data_maps = json.load(f)\n",
    "\n",
    "        for key,value in tqdm(data_maps['id2meta'].items()):\n",
    "            new_value='<extra_id_1>'+value+'<extra_id_2>'\n",
    "            #tokenize it\n",
    "            tokenized_value=tokenizer.tokenize(new_value)\n",
    "            if len(tokenized_value)<=255:\n",
    "                data_maps['id2meta'][key]=new_value\n",
    "            else:\n",
    "                diff=len(tokenized_value)-255\n",
    "                new_value='<extra_id_1>'+value[:-diff]+'<extra_id_2>'\n",
    "                data_maps['id2meta'][key]=new_value\n",
    "\n",
    "        # Load test data\n",
    "        df=pd.read_csv(os.path.join(base_dir, domain, f\"{domain}.test.csv\"))\n",
    "        df['count']=df['history_text'].apply(lambda x:x.count('<extra_id_1>'))\n",
    "        count_values = df['count'].value_counts().sort_index()\n",
    "\n",
    "        history_count_10=[]\n",
    "        history_count_50=[]\n",
    "        for i,count in enumerate(count_values[:7]):\n",
    "            print(i+1,count)\n",
    "            if (i+1>10):\n",
    "                break\n",
    "            temp=df[df['count']==i+1]\n",
    "\n",
    "            queries = {}\n",
    "            relevant_docs = {}\n",
    "            for _, row in temp.iterrows():\n",
    "                user_id = str(data_maps['user2id'][row['user_id']])\n",
    "                item_id = str(data_maps['item2id'][row['parent_asin']])\n",
    "\n",
    "                if user_id not in queries:\n",
    "                    queries[user_id] = row['history_text']\n",
    "                    relevant_docs[user_id] = []\n",
    "\n",
    "                relevant_docs[user_id].append(item_id)\n",
    "\n",
    "            # Create and run evaluator\n",
    "            ir_evaluator = InformationRetrievalEvaluator(\n",
    "                queries=queries,\n",
    "                corpus=data_maps['id2meta'],\n",
    "                relevant_docs=relevant_docs,\n",
    "                ndcg_at_k=[1,10,50],\n",
    "                precision_recall_at_k=[1,10,50],\n",
    "                show_progress_bar=True,\n",
    "                batch_size=32,\n",
    "            )\n",
    "            results = ir_evaluator(model)\n",
    "            print(f\"{domain}: NDCG@10:{results['cosine_ndcg@10']:.4f} NDGC@50:{results['cosine_ndcg@50']:.4f} Recall@10:{results['cosine_recall@10']:.4f} Recall@50:{results['cosine_recall@50']:.4f}\")\n",
    "            history_count_10.append(results['cosine_ndcg@10'])\n",
    "            history_count_50.append(results['cosine_ndcg@50'])\n",
    "        all_results[domain][model_name]['ndcg@10'] = history_count_10\n",
    "        all_results[domain][model_name]['ndcg@50'] = history_count_50\n",
    "        \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
